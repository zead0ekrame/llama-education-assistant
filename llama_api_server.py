# ğŸš€ Llama 3.1 API Server Ù„Ù„Ù†Ø³Ø® ÙÙŠ Colab
# Ø§Ù†Ø³Ø® Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ Google Colab

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
"""
%pip install transformers torch accelerate bitsandbytes
%pip install flask flask-cors pyngrok
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
import threading
import json
from datetime import datetime
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ Hugging Face Token
HF_TOKEN = "YOUR_HUGGING_FACE_TOKEN_HERE"  # Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ù‡Ù†Ø§
MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct"

print(f"ğŸ¤— Hugging Face Token: ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆÙƒÙ† ÙÙŠ Ø§Ù„ÙƒÙˆØ¯")
print(f"ğŸ¤– Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1
def load_llama_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1 Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1...")
    
    # ØªØ­Ù…ÙŠÙ„ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        token=HF_TOKEN,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        load_in_8bit=True  # Ø¶ØºØ· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„Ù„ØªÙØ§Ø¹Ù„
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
    return pipe, tokenizer

# Ø¯Ø§Ù„Ø© Ù„Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
def chat_with_llama(user_input, max_length=512):
    """Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ prompt Ù„Ù„ØªØ¹Ù„ÙŠÙ…
    system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ØªØ¹Ù„ÙŠÙ…ÙŠ Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…. 
    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙŠØ¯Ø©. 
    Ø§Ø³ØªØ®Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©."""
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Llama 3.1 Ø§Ù„ØµØ­ÙŠØ­
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        inputs = tokenizer(prompt, return_tensors="pt").to(pipe.device)
        
        with torch.no_grad():
            outputs = pipe.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        return response.strip()
    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("ğŸ”„ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
pipe, tokenizer = load_llama_model()

# Ø¥Ù†Ø´Ø§Ø¡ Flask API
app = Flask(__name__)
CORS(app)

@app.route('/', methods=['GET'])
def home():
    return jsonify({
        'status': 'success',
        'message': 'Llama 3.1 API is running!',
        'model': MODEL_NAME,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/chat', methods=['POST'])
def chat():
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø·Ù„Ø¨
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({
                'success': False,
                'error': 'Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø©',
                'status': 'error'
            }), 400
        
        user_input = data['message'].strip()
        if not user_input:
            return jsonify({
                'success': False,
                'error': 'Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙØ§Ø±ØºØ©',
                'status': 'error'
            }), 400
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        response = chat_with_llama(user_input)
        
        return jsonify({
            'success': True,
            'response': response,
            'user_input': user_input,
            'model': MODEL_NAME,
            'timestamp': datetime.now().isoformat(),
            'status': 'success'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'healthy',
        'model_loaded': True,
        'timestamp': datetime.now().isoformat()
    })

# ØªØ´ØºÙŠÙ„ API Ù…Ø¹ ngrok
def run_flask():
    app.run(host='0.0.0.0', port=5000, debug=False)

# ØªØ´ØºÙŠÙ„ Flask ÙÙŠ thread Ù…Ù†ÙØµÙ„
flask_thread = threading.Thread(target=run_flask)
flask_thread.daemon = True
flask_thread.start()

# Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„ Ø­ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø®Ø§Ø¯Ù…
time.sleep(3)

# Ø¥Ù†Ø´Ø§Ø¡ ngrok tunnel
try:
    public_url = ngrok.connect(5000)
    print("ğŸ‰ ØªÙ… ØªØ´ØºÙŠÙ„ API Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ğŸŒ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…: {public_url.public_url}")
    print(f"ğŸ”— Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø­Ù„ÙŠ")
    print("\nğŸ“‹ Ø·Ø±Ù‚ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:")
    print(f"GET  {public_url.public_url}/        - Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")
    print(f"POST {public_url.public_url}/chat    - Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø©")
    print(f"GET  {public_url.public_url}/health  - ÙØ­Øµ Ø§Ù„Ø­Ø§Ù„Ø©")
    print("\nğŸ’¡ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:")
    print(f"curl -X POST {public_url.public_url}/chat \\")
    print('     -H "Content-Type: application/json" \\')
    print('     -d \'{"message": "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ?"}\'\n')
    
    # Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ Ù…Ù„Ù
    with open('/content/api_url.txt', 'w') as f:
        f.write(public_url.public_url)
    print("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ api_url.txt")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ ngrok: {e}")

# Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªØ´ØºÙŠÙ„ API
print("\nğŸ”„ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªØ´ØºÙŠÙ„ API:")
print("Ø´ØºÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ø°Ø§ ØªÙˆÙ‚Ù ngrok:")
print("""
try:
    ngrok.disconnect(public_url.public_url)
except:
    pass

public_url = ngrok.connect(5000)
print(f"ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ ngrok: {public_url.public_url}")
""")
