# ğŸ“ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ - Llama 3.1
# ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ø³ØªÙˆØ±Ø¯Ø© Ù…Ù† Flowise

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
import subprocess
import sys

def install_packages():
    """ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    packages = [
        "transformers",
        "torch", 
        "accelerate",
        "bitsandbytes",
        "openai-whisper",
        "gtts",
        "gradio",
        "huggingface_hub",
        "pydub"
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… ØªÙ… ØªØ«Ø¨ÙŠØª {package}")
        except:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ«Ø¨ÙŠØª {package}")

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
print("ğŸ”„ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...")
install_packages()

# Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import whisper
from gtts import gTTS
import gradio as gr
import tempfile
import os
from datetime import datetime
from pydub import AudioSegment

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
HF_TOKEN = "YOUR_HUGGING_FACE_TOKEN_HERE"  # Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ù‡Ù†Ø§
MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # Ù†Ù…ÙˆØ°Ø¬ Llama 3.1

print(f"ğŸ¤— Hugging Face Token: ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆÙƒÙ† ÙÙŠ Ø§Ù„ÙƒÙˆØ¯")
print(f"ğŸ¤– Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1
def load_llama_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1 Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1...")
    
    # ØªØ­Ù…ÙŠÙ„ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        token=HF_TOKEN,  # Ù‡Ù†Ø§ Ø§Ù„ØªÙˆÙƒÙ†
        trust_remote_code=True
    )
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        token=HF_TOKEN,  # Ù‡Ù†Ø§ Ø§Ù„ØªÙˆÙƒÙ†
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„Ù„ØªÙØ§Ø¹Ù„
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
    return pipe, tokenizer

# Ø¯Ø§Ù„Ø© Ù„Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
def chat_with_llama(pipe, tokenizer, user_input, max_length=512):
    """Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Llama 3.1"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ prompt Ù„Ù„ØªØ¹Ù„ÙŠÙ…
    system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ØªØ¹Ù„ÙŠÙ…ÙŠ Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…. 
    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙŠØ¯Ø©. 
    Ø§Ø³ØªØ®Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©."""
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Llama 3.1 Ø§Ù„ØµØ­ÙŠØ­
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    inputs = tokenizer(prompt, return_tensors="pt").to(pipe.device)
    
    with torch.no_grad():
        outputs = pipe.model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    return response.strip()

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper
print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper...")
whisper_model = whisper.load_model("base")
print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Whisper Ø¨Ù†Ø¬Ø§Ø­!")

# Ø¯ÙˆØ§Ù„ TTS Ùˆ STT
def text_to_speech(text, language='ar'):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…"""
    try:
        tts = gTTS(text=text, lang=language, slow=False)
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ù…Ø¤Ù‚ØªØ§Ù‹
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
            tts.save(tmp_file.name)
            return tmp_file.name
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ TTS: {e}")
        return None

def speech_to_text(audio_file):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ"""
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ WAV Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        audio = AudioSegment.from_file(audio_file)
        wav_file = audio_file.replace('.mp3', '.wav').replace('.m4a', '.wav')
        audio.export(wav_file, format="wav")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
        result = whisper_model.transcribe(wav_file, language="ar")
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        if os.path.exists(wav_file):
            os.remove(wav_file)
            
        return result["text"].strip()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ STT: {e}")
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ÙÙ‡Ù… Ø§Ù„ØµÙˆØª"

def voice_chat(pipe, tokenizer, audio_file):
    """Ù…Ø­Ø§Ø¯Ø«Ø© ØµÙˆØªÙŠØ© ÙƒØ§Ù…Ù„Ø©"""
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
    user_text = speech_to_text(audio_file)
    
    if not user_text:
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ÙÙ‡Ù… Ø§Ù„ØµÙˆØª", "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ÙÙ‡Ù… Ø§Ù„ØµÙˆØª", None
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    response = chat_with_llama(pipe, tokenizer, user_text)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ ØµÙˆØª
    audio_response = text_to_speech(response)
    
    return user_text, response, audio_response

# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ø³ØªÙˆØ±Ø¯Ø© Ù…Ù† Flowise
def create_education_interface():
    """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù„Ù„ØªØ¹Ù„ÙŠÙ…"""
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    pipe, tokenizer = load_llama_model()
    
    def process_text_input(user_input):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
        if not user_input.strip():
            return "Ù…Ù† ÙØ¶Ù„Ùƒ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ"
        
        response = chat_with_llama(pipe, tokenizer, user_input)
        return response
    
    def process_voice_input(audio_file):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø¯Ø®Ù„"""
        if audio_file is None:
            return "Ù…Ù† ÙØ¶Ù„Ùƒ Ø³Ø¬Ù„ ØµÙˆØªÙƒ", None
        
        user_text, response, audio_response = voice_chat(pipe, tokenizer, audio_file)
        
        return f"ğŸ¤ Ø³Ø¤Ø§Ù„Ùƒ: {user_text}\n\nğŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response}", audio_response
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù…Ø­Ø³Ù†Ø©
    with gr.Blocks(
        title="Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ - ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø³Ù†Ø©",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
            margin: auto !important;
        }
        .main-header {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 2rem;
        }
        .chat-message {
            padding: 1rem;
            border-radius: 15px;
            margin: 1rem 0;
            max-width: 80%;
        }
        .user-message {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
        }
        .bot-message {
            background: white;
            color: #333;
            border: 1px solid #e0e0e0;
            margin-right: auto;
        }
        """
    ) as interface:
        
        # Header Ù…Ø­Ø³Ù†
        gr.Markdown("""
        <div class="main-header">
            <h1>ğŸ“ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ</h1>
            <p>ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù…Ø³ØªÙˆØ±Ø¯Ø© Ù…Ù† Flowise - ØªÙØ§Ø¹Ù„ Ø°ÙƒÙŠ Ù…Ø¹ Ø§Ù„Ø·Ù„Ø§Ø¨</p>
        </div>
        """)
        
        # Tabs Ù…Ø­Ø³Ù†Ø©
        with gr.Tabs():
            with gr.Tab("ğŸ’¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ©"):
                gr.Markdown("### ğŸ’¬ ØªÙØ§Ø¹Ù„ Ù†ØµÙŠ Ø°ÙƒÙŠ")
                
                with gr.Row():
                    with gr.Column(scale=3):
                        text_input = gr.Textbox(
                            label="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§",
                            placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
                            lines=3
                        )
                        text_btn = gr.Button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„", variant="primary", size="lg")
                    
                    with gr.Column(scale=2):
                        text_output = gr.Textbox(
                            label="ğŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©",
                            lines=10,
                            interactive=False
                        )
                
                text_btn.click(
                    process_text_input,
                    inputs=text_input,
                    outputs=text_output
                )
            
            with gr.Tab("ğŸ¤ Ù…Ø­Ø§Ø¯Ø«Ø© ØµÙˆØªÙŠØ©"):
                gr.Markdown("### ğŸ¤ ØªÙØ§Ø¹Ù„ ØµÙˆØªÙŠ Ù…ØªÙ‚Ø¯Ù…")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        audio_input = gr.Audio(
                            label="Ø³Ø¬Ù„ ØµÙˆØªÙƒ",
                            type="filepath",
                            format="mp3"
                        )
                        voice_btn = gr.Button("ğŸ¤ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª", variant="primary", size="lg")
                    
                    with gr.Column(scale=3):
                        voice_output = gr.Textbox(
                            label="Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©",
                            lines=8,
                            interactive=False
                        )
                        audio_output = gr.Audio(
                            label="Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµÙˆØªÙŠØ©",
                            type="filepath"
                        )
                
                voice_btn.click(
                    process_voice_input,
                    inputs=audio_input,
                    outputs=[voice_output, audio_output]
                )
            
            with gr.Tab("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"):
                gr.Markdown("### ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("""
                        **ğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
                        - Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©
                        - ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ ØªØ¹Ù„ÙŠÙ…ÙŠ
                        - Ø¬Ø±Ø¨ Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„
                        - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
                        """)
                    
                    with gr.Column():
                        gr.Markdown("""
                        **ğŸ¯ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©:**
                        - ØªØµÙ…ÙŠÙ… Ø§Ø­ØªØ±Ø§ÙÙŠ Ù…Ø³ØªÙˆØ±Ø¯ Ù…Ù† Flowise
                        - Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
                        - Ø£Ø²Ø±Ø§Ø± Ø³Ø±ÙŠØ¹Ø© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
                        - ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨ØµÙŠØºØ© JSON
                        """)
        
        # Footer
        gr.Markdown("""
        ---
        **ğŸ“ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ** | ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ø³ØªÙˆØ±Ø¯Ø© Ù…Ù† Flowise
        """)
    
    return interface

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == "__main__":
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ…...")
    print(f"â° Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ: {datetime.now()}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
    interface = create_education_interface()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
    interface.launch(
        share=True,
        debug=True,
        server_name="0.0.0.0",
        server_port=7860
    )
    
    print("ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù…ØªØ§Ø­ Ø§Ù„Ø¢Ù†!")
    print("ğŸ’¡ Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ Ø´ØºÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ø±Ø© Ø£Ø®Ø±Ù‰")
